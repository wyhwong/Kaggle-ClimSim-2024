{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from lightning import LightningDataModule\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.utils import check_and_create_dir\n",
    "from src.pytorch.models.utils import get_default_trainer\n",
    "from src.pytorch.models.mlp import DynamicMLP\n",
    "from src.pytorch.models.fastkan import FastKAN\n",
    "from src.pytorch.data.parquet import Dataset\n",
    "from src.pytorch.loss.r2 import r2_score_multivariate, r2_loss\n",
    "from src.schemas.climsim import INPUT_COLUMNS, OUTPUT_COLUMNS\n",
    "from src.visualization.performance import loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINSET_DATA_PATH = \"/home/data/subset_train.parquet\"\n",
    "TRAINSET_DATA_PATH = \"/home/data/train.parquet\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "check_and_create_dir(OUTPUT_DIR)\n",
    "MODEL_NAME = \"climsim_best_model\"\n",
    "\n",
    "# Common parameters\n",
    "BATCH_SIZE = 3072\n",
    "N_EPOCHS = 100\n",
    "# Given TRAINING_SAMPLE_FRAC=0.7, 70% of the samples will be used for training\n",
    "TRAINING_SAMPLE_FRAC = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# BUFFER_SIZE: number of batches being preloaded in memory\n",
    "TRAINING_BUFFER_SIZE = 100\n",
    "# N_GROUP_PER_SAMPLING: number of groups being sampled in each iteration\n",
    "TRAINING_N_GROUP_PER_SAMPLING = 3\n",
    "# N_BATCH_PER_SAMPLING: number of batches being sampled in each iteration\n",
    "TRAINING_N_BATCH_PER_SAMPLING = 100\n",
    "\n",
    "# Validation parameters\n",
    "VALIDATION_BUFFER_SIZE = 100\n",
    "VALIDATION_N_GROUP_PER_SAMPLING = 2\n",
    "VALIDATION_N_BATCH_PER_SAMPLING = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimSimDataModule(LightningDataModule):\n",
    "    \"\"\" Data module for the ClimSim dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, batch_size: int) -> None:\n",
    "        \"\"\" Initialize the data module.\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self._data_path = data_path\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # Use full dataset for training\n",
    "        parquet = pq.ParquetFile(self._data_path, memory_map=True, buffer_size=10)\n",
    "        all_groups = list(range(0, parquet.num_row_groups))\n",
    "        train_groups = random.sample(all_groups, int(TRAINING_SAMPLE_FRAC * len(all_groups)))\n",
    "        val_groups = list(set(all_groups) - set(train_groups))\n",
    "\n",
    "        # Use one group for testing\n",
    "        # train_groups = [0]\n",
    "        # val_groups = [0]\n",
    "\n",
    "        self.train = Dataset(\n",
    "            source=self._data_path,\n",
    "            input_cols=INPUT_COLUMNS,\n",
    "            target_cols=OUTPUT_COLUMNS,\n",
    "            batch_size=self._batch_size,\n",
    "            buffer_size=TRAINING_BUFFER_SIZE,\n",
    "            groups=train_groups,\n",
    "            n_group_per_sampling=TRAINING_N_GROUP_PER_SAMPLING,\n",
    "            n_batch_per_sampling=TRAINING_N_BATCH_PER_SAMPLING,\n",
    "            to_tensor=True,\n",
    "            normalize=True,\n",
    "        )\n",
    "        self.val = Dataset(\n",
    "            source=self._data_path,\n",
    "            input_cols=INPUT_COLUMNS,\n",
    "            target_cols=OUTPUT_COLUMNS,\n",
    "            batch_size=self._batch_size,\n",
    "            buffer_size=VALIDATION_BUFFER_SIZE,\n",
    "            groups=val_groups,\n",
    "            n_group_per_sampling=VALIDATION_N_GROUP_PER_SAMPLING,\n",
    "            n_batch_per_sampling=VALIDATION_N_BATCH_PER_SAMPLING,\n",
    "            to_tensor=True,\n",
    "            normalize=True,\n",
    "        )\n",
    "\n",
    "        self.train.start_workers()\n",
    "        self.val.start_workers()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train.to_dataloader()\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val.to_dataloader()\n",
    "\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        self.train.shutdown_workers()\n",
    "        self.train.clean_up()\n",
    "        self.val.shutdown_workers()\n",
    "        self.val.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DynamicMLP(\n",
    "#     loss_train=r2_score_multivariate,\n",
    "#     loss_val=r2_score_multivariate,\n",
    "# )\n",
    "\n",
    "model = FastKAN(\n",
    "    layers_hidden=[556, 556, 368],\n",
    "    loss_train=r2_loss,\n",
    "    loss_val=r2_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = ClimSimDataModule(TRAINSET_DATA_PATH, BATCH_SIZE)\n",
    "\n",
    "trainer = get_default_trainer(\n",
    "    model_name=\"climsim-2024\",\n",
    "    max_epochs=N_EPOCHS,\n",
    "    max_time=datetime.timedelta(hours=1),\n",
    "    check_val_every_n_epoch=3,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = model.get_epoch_loss()\n",
    "ds_train_loss = pd.Series(train_loss, name=\"Train Loss\")\n",
    "ds_val_loss = pd.Series(val_loss, name=\"Validation Loss\")\n",
    "loss_curve(\n",
    "    losses=[ds_train_loss, ds_val_loss],\n",
    "    close=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
