{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import src.pytorch.data.kaggle\n",
    "import src.pytorch.models\n",
    "import src.pytorch.models.utils\n",
    "import src.visualization.performance\n",
    "\n",
    "from src.pytorch.data.parquet import Dataset\n",
    "from src.schemas.climsim import INPUT_COLUMNS, OUTPUT_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINSET_DATA_PATH = \"/home/data/subset_train.arrow\"\n",
    "TRAINSET_DATA_PATH = \"/home/data/train.parquet\"\n",
    "\n",
    "# Common parameters\n",
    "BATCH_SIZE = 3072\n",
    "N_EPOCHS = 10\n",
    "# Given TRAINING_SAMPLE_FRAC=0.7, 70% of the samples will be used for training\n",
    "TRAINING_SAMPLE_FRAC = 0.7\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_BUFFER_SIZE = 480\n",
    "TRAINING_N_GROUP_PER_SAMPLING = 3\n",
    "TRAINING_N_BATCH_PER_SAMPLING = 120\n",
    "\n",
    "# Validation parameters\n",
    "VALIDATION_BUFFER_SIZE = 320\n",
    "VALIDATION_N_GROUP_PER_SAMPLING = 2\n",
    "VALIDATION_N_BATCH_PER_SAMPLING = 80\n",
    "\n",
    "# Buffer size calculation\n",
    "# Max GPU memory usage = buffer_size * batch_size * num_columns * 4 bits\n",
    "#                      = (480 + 320) * 3072 * 1000 * 4\n",
    "#                      = 9830400000 bits\n",
    "#                      = 9.16 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.scan_parquet(TRAINSET_DATA_PATH)\n",
    "n_samples = lf.select(pl.len()).collect().item()\n",
    "\n",
    "parquet = pq.ParquetFile(TRAINSET_DATA_PATH, memory_map=True, buffer_size=10)\n",
    "all_groups = list(range(0, parquet.num_row_groups))\n",
    "train_groups = random.sample(all_groups, int(TRAINING_SAMPLE_FRAC * len(all_groups)))\n",
    "val_groups = list(set(all_groups) - set(train_groups))\n",
    "\n",
    "dataset_train = Dataset(\n",
    "    parquet=parquet,\n",
    "    input_cols=INPUT_COLUMNS,\n",
    "    target_cols=OUTPUT_COLUMNS,\n",
    "    n_samples=int(n_samples / len(all_groups) * len(train_groups)),\n",
    "    groups=train_groups,\n",
    "    buffer_size=TRAINING_BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_group_per_sampling=TRAINING_N_GROUP_PER_SAMPLING,\n",
    "    n_batch_per_sampling=TRAINING_N_BATCH_PER_SAMPLING,\n",
    ")\n",
    "dataset_val = Dataset(\n",
    "    parquet=parquet,\n",
    "    input_cols=INPUT_COLUMNS,\n",
    "    target_cols=OUTPUT_COLUMNS,\n",
    "    n_samples=int(n_samples / len(all_groups) * len(val_groups)),\n",
    "    groups=val_groups,\n",
    "    buffer_size=VALIDATION_BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_group_per_sampling=VALIDATION_N_GROUP_PER_SAMPLING,\n",
    "    n_batch_per_sampling=VALIDATION_N_BATCH_PER_SAMPLING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = src.pytorch.models.MLP()\n",
    "\n",
    "trainloader = dataset_train.to_dataloader()\n",
    "valloader = dataset_val.to_dataloader()\n",
    "\n",
    "model, best_weights, loss = src.pytorch.models.utils.train(\n",
    "    model=model,\n",
    "    dataloaders={\"Training\": trainloader, \"Validation\": valloader},\n",
    "    n_epochs=N_EPOCHS,\n",
    ")\n",
    "src.visualization.performance.loss_curve(loss, close=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights if needed\n",
    "MODEL_WEIGHTS_PATH = \"./model.pt\"\n",
    "\n",
    "src.pytorch.models.utils.save_weights(\n",
    "    model=model,\n",
    "    model_path=MODEL_WEIGHTS_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to generate submission file\n",
    "\n",
    "TESTSET_DATA_PATH = \"/home/data/test.arrow\"\n",
    "TESTSET_PREDICTION_WEIGHTS_PATH = \"/home/data/sample_submission.arrow\"\n",
    "\n",
    "# df_submission = pl.read_ipc(TESTSET_DATA_PATH).to_pandas()\n",
    "# df_weights = pl.read_ipc(TESTSET_PREDICTION_WEIGHTS_PATH).to_pandas()\n",
    "\n",
    "# Load model to generate submission file\n",
    "\n",
    "# model = src.pytorch.models.MLP()\n",
    "# src.pytorch.models.utils.load_model(model, MODEL_WEIGHTS_PATH)\n",
    "# model.to(src.env.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to submit the predictions\n",
    "\n",
    "# src.pytorch.data.kaggle.output_compressed_parquet(\n",
    "#     model=model,\n",
    "#     df=df_submission,\n",
    "#     weights=df_weights,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
